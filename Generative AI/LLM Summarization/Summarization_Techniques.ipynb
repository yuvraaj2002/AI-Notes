{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !pip install langchain-groq\n",
        "# !pip install langchain\n",
        "# !pip install -U langchain-community\n",
        "# !pip install langchain-experimental"
      ],
      "metadata": {
        "id": "RoP0Q_d0-HXa"
      },
      "id": "RoP0Q_d0-HXa",
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from rich import print\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
        "from google.colab import userdata\n",
        "from langchain import PromptTemplate\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "print(\"Everything Imported succesfully✅\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Y9hgzqhS9pZD",
        "outputId": "260e628c-9518-4b02-9173-5c5609935acc"
      },
      "id": "Y9hgzqhS9pZD",
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Everything Imported succesfully✅\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Everything Imported succesfully✅\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_content(url:str):\n",
        "\n",
        "  # Send a GET request to the webpage\n",
        "  response = requests.get(url)\n",
        "\n",
        "  # Check if the request was successful\n",
        "  if response.status_code == 200:\n",
        "      # Parse the HTML content using BeautifulSoup\n",
        "      soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "      # Extract the main content (text)\n",
        "      # This site has the main text within <font> tags\n",
        "      content = soup.find_all(\"font\")\n",
        "\n",
        "      # Combine and clean the extracted text\n",
        "      full_text = \"\\n\".join([element.get_text() for element in content])\n",
        "\n",
        "      return full_text\n",
        "  else:\n",
        "      print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n"
      ],
      "metadata": {
        "id": "rFagfbe9Jde_"
      },
      "id": "rFagfbe9Jde_",
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "4033ee5a",
      "metadata": {
        "id": "4033ee5a"
      },
      "outputs": [],
      "source": [
        "# Loading the LLM model\n",
        "os.environ[\"GROQ_API_KEY\"] = userdata.get('GROQ_API_KEY')\n",
        "model = ChatGroq(model=\"llama3-8b-8192\")\n",
        "\n",
        "# Embedding Model\n",
        "embedding_model = HuggingFaceInferenceAPIEmbeddings(api_key =userdata.get('HF_TOKEN'),model_name=\"BAAI/bge-base-en-v1.5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13f1cf66",
      "metadata": {
        "id": "13f1cf66"
      },
      "source": [
        "## Technique 1: Using Prompt Templates\n",
        "\n",
        "Prompt templates are a great way to dynamically place text within your prompts. They are like [python f-strings](https://realpython.com/python-f-strings/) but specialized for working with language models.\n",
        "\n",
        "We're going to look at 2 short Paul Graham essays"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "id": "b326aaeb",
      "metadata": {
        "id": "b326aaeb"
      },
      "outputs": [],
      "source": [
        "url = \"https://paulgraham.com/persistence.html\"\n",
        "essay = get_content(url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "id": "946c379a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "946c379a",
        "outputId": "d12be5c1-a22b-40cd-c676-6f7065410afb"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Summary: Here is a \u001b[1;36m3\u001b[0m sentence summary of the text:\n",
              "\n",
              "The author argues that while persistence and obstinacy may seem similar, they are distinct behaviors. Persistence \n",
              "is a complex phenomenon that involves energy, imagination, resilience, good judgment, and a focus on a goal, \n",
              "whereas obstinacy is a simple and often misguided resistance to change. The author contends that persistence is a \n",
              "valuable trait that can lead to success, whereas obstinacy is often a recipe for failure.\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Summary: Here is a <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> sentence summary of the text:\n",
              "\n",
              "The author argues that while persistence and obstinacy may seem similar, they are distinct behaviors. Persistence \n",
              "is a complex phenomenon that involves energy, imagination, resilience, good judgment, and a focus on a goal, \n",
              "whereas obstinacy is a simple and often misguided resistance to change. The author contends that persistence is a \n",
              "valuable trait that can lead to success, whereas obstinacy is often a recipe for failure.\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m{\u001b[0m\n",
              "    \u001b[32m'token_usage'\u001b[0m: \u001b[1m{\u001b[0m\n",
              "        \u001b[32m'completion_tokens'\u001b[0m: \u001b[1;36m94\u001b[0m,\n",
              "        \u001b[32m'prompt_tokens'\u001b[0m: \u001b[1;36m2667\u001b[0m,\n",
              "        \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m2761\u001b[0m,\n",
              "        \u001b[32m'completion_time'\u001b[0m: \u001b[1;36m0.078333333\u001b[0m,\n",
              "        \u001b[32m'prompt_time'\u001b[0m: \u001b[1;36m0.309614334\u001b[0m,\n",
              "        \u001b[32m'queue_time'\u001b[0m: \u001b[1;36m0.0021887649999999814\u001b[0m,\n",
              "        \u001b[32m'total_time'\u001b[0m: \u001b[1;36m0.387947667\u001b[0m\n",
              "    \u001b[1m}\u001b[0m,\n",
              "    \u001b[32m'model_name'\u001b[0m: \u001b[32m'llama3-8b-8192'\u001b[0m,\n",
              "    \u001b[32m'system_fingerprint'\u001b[0m: \u001b[32m'fp_6a6771ae9c'\u001b[0m,\n",
              "    \u001b[32m'finish_reason'\u001b[0m: \u001b[32m'stop'\u001b[0m,\n",
              "    \u001b[32m'logprobs'\u001b[0m: \u001b[3;35mNone\u001b[0m\n",
              "\u001b[1m}\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'token_usage'</span>: <span style=\"font-weight: bold\">{</span>\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'completion_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">94</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2667</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2761</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'completion_time'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.078333333</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_time'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.309614334</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'queue_time'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0021887649999999814</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'total_time'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.387947667</span>\n",
              "    <span style=\"font-weight: bold\">}</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'model_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'llama3-8b-8192'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'system_fingerprint'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'fp_6a6771ae9c'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'finish_reason'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'logprobs'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
              "<span style=\"font-weight: bold\">}</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 26.5 ms, sys: 1.91 ms, total: 28.4 ms\n",
            "Wall time: 612 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Generate the summary\n",
        "summary = model.invoke(f\"Please write a 3 sentence summary of the following text:{essay}\")\n",
        "\n",
        "# Print the summary\n",
        "print(f\"Summary: {summary.content}\\n\")\n",
        "print(summary.response_metadata)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49ae9a41",
      "metadata": {
        "id": "49ae9a41"
      },
      "source": [
        "## Level 3: Map Reduce - Summarize a couple pages multiple pages\n",
        "\n",
        "If you have multiple pages you'd like to summarize, you'll likely run into a token limit. Token limits won't always be a problem, but it is good to know how to handle them if you run into the issue.\n",
        "\n",
        "The chain type \"Map Reduce\" is a method that helps with this. You first generate a summary of smaller chunks (that fit within the token limit) and then you get a summary of the summaries.\\\n",
        "\n",
        "Check out [this video](https://www.youtube.com/watch?v=f9_BWhCI4Zo) for more information on how chain types work"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "id": "c2ce6270",
      "metadata": {
        "id": "c2ce6270"
      },
      "outputs": [],
      "source": [
        "text_splitter_semantic = SemanticChunker(\n",
        "    embedding_model, breakpoint_threshold_type=\"percentile\" # \"standard_deviation\", \"interquartile\"\n",
        ")\n",
        "\n",
        "text_splitter_recursive = RecursiveCharacterTextSplitter(separators=[\"\\n\\n\", \"\\n\"], chunk_size=5000, chunk_overlap=1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb6de34d",
      "metadata": {
        "id": "bb6de34d"
      },
      "source": [
        "Let's see how many tokens are in combined essays"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "id": "b133938a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "id": "b133938a",
        "outputId": "e9326cdd-0bb3-414b-ca89-1a17cb068cf5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Total tokens: \u001b[1;36m10828\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Total tokens: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10828</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "essays = \"\"\n",
        "for urls in ['https://paulgraham.com/persistence.html','https://paulgraham.com/reddits.html',\n",
        "             'https://paulgraham.com/greatwork.html','https://paulgraham.com/donate.html']:\n",
        "  essays = essays + get_content(url)\n",
        "\n",
        "print(\"Total tokens:\",model.get_num_tokens(essays))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc8ec39d",
      "metadata": {
        "id": "dc8ec39d"
      },
      "source": [
        "That's too many, let's split our text up into chunks so they fit into the prompt limit. I'm going a chunk size of 10,000 characters.\n",
        "\n",
        "> You can think of tokens as pieces of words used for natural language processing. For English text, **1 token is approximately 4 characters** or 0.75 words. As a point of reference, the collected works of Shakespeare are about 900,000 words or 1.2M tokens.\n",
        "\n",
        "This means the number of tokens we should expect is 10,000 / 4 = ~2,500 token chunks. But this will vary, each body of text/code will be different"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "id": "2e7c372b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2e7c372b",
        "outputId": "8436888d-96ae-4cf4-e2bd-55c0b9445f56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 331 ms, sys: 26.8 ms, total: 358 ms\n",
            "Wall time: 1.44 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "docs_semantic = text_splitter_semantic.create_documents([essays])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "docs_recursive = text_splitter_recursive.create_documents([essays])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-gV1u6YLuN0",
        "outputId": "559e946a-40b4-4e41-887c-0ca74e8a59ee"
      },
      "id": "e-gV1u6YLuN0",
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1.81 ms, sys: 0 ns, total: 1.81 ms\n",
            "Wall time: 1.82 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chunks_length_semantic = [len(docs.page_content) for docs in docs_semantic]\n",
        "chunks_length_recursive = [len(docs.page_content) for docs in docs_recursive]\n",
        "\n",
        "# Ensure both datasets are of equal length for comparison\n",
        "chunk_indices = list(range(1, max(len(chunks_length_semantic), len(chunks_length_recursive)) + 1))\n",
        "\n",
        "# Create the bar chart\n",
        "fig = go.Figure()\n",
        "\n",
        "# Add the first trace for semantic chunks\n",
        "fig.add_trace(\n",
        "    go.Bar(\n",
        "        x=chunk_indices[:len(chunks_length_semantic)],  # Indices for the semantic chunks\n",
        "        y=chunks_length_semantic,  # Lengths of the semantic chunks\n",
        "        name=\"Semantic Chunks\",  # Legend name\n",
        "        marker_color=\"royalblue\"  # Color of the bars\n",
        "    )\n",
        ")\n",
        "\n",
        "# Add the second trace for recursive chunks\n",
        "fig.add_trace(\n",
        "    go.Bar(\n",
        "        x=chunk_indices[:len(chunks_length_recursive)],  # Indices for the recursive chunks\n",
        "        y=chunks_length_recursive,  # Lengths of the recursive chunks\n",
        "        name=\"Recursive Chunks\",  # Legend name\n",
        "        marker_color=\"darkorange\"  # Color of the bars\n",
        "    )\n",
        ")\n",
        "\n",
        "# Customize the layout\n",
        "fig.update_layout(\n",
        "    barmode='group',  # Group the bars side by side\n",
        "    xaxis_title=\"Chunk Index\",\n",
        "    yaxis_title=\"Chunk Length (Characters)\",\n",
        "    title=\"Comparison of Chunk Lengths: Semantic vs Recursive\",\n",
        "    hovermode=\"x unified\",\n",
        "    template=\"plotly_dark\",  # You can change this to 'plotly' for a light theme\n",
        "    legend_title_text=\"Chunk Types\"  # Title for the legend\n",
        ")\n",
        "\n",
        "# Show the plot\n",
        "fig.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "NEMtRRXzLzNz",
        "outputId": "3e9664ac-67bb-4009-cbd0-b68158e9c28e"
      },
      "id": "NEMtRRXzLzNz",
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"b34e4a38-eb76-4bcf-96ef-f466d9eda084\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"b34e4a38-eb76-4bcf-96ef-f466d9eda084\")) {                    Plotly.newPlot(                        \"b34e4a38-eb76-4bcf-96ef-f466d9eda084\",                        [{\"marker\":{\"color\":\"royalblue\"},\"name\":\"Semantic Chunks\",\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21],\"y\":[1405,1137,27,4138,3785,2479,1137,27,4138,3785,2479,1137,27,4138,3785,2479,1137,27,4138,3785,1074],\"type\":\"bar\"},{\"marker\":{\"color\":\"darkorange\"},\"name\":\"Recursive Chunks\",\"x\":[1,2,3,4,5,6,7,8,9,10,11,12],\"y\":[4988,4982,4998,4928,4978,4945,4908,4999,4959,4931,4970,2299],\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"rgb(17,17,17)\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#f2f5fa\"},\"error_y\":{\"color\":\"#f2f5fa\"},\"marker\":{\"line\":{\"color\":\"rgb(17,17,17)\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#A2B1C6\",\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"minorgridcolor\":\"#506784\",\"startlinecolor\":\"#A2B1C6\"},\"baxis\":{\"endlinecolor\":\"#A2B1C6\",\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"minorgridcolor\":\"#506784\",\"startlinecolor\":\"#A2B1C6\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"line\":{\"color\":\"#283442\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"marker\":{\"line\":{\"color\":\"#283442\"}},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#506784\"},\"line\":{\"color\":\"rgb(17,17,17)\"}},\"header\":{\"fill\":{\"color\":\"#2a3f5f\"},\"line\":{\"color\":\"rgb(17,17,17)\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#f2f5fa\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#f2f5fa\"},\"geo\":{\"bgcolor\":\"rgb(17,17,17)\",\"lakecolor\":\"rgb(17,17,17)\",\"landcolor\":\"rgb(17,17,17)\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#506784\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"dark\"},\"paper_bgcolor\":\"rgb(17,17,17)\",\"plot_bgcolor\":\"rgb(17,17,17)\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"ticks\":\"\"},\"bgcolor\":\"rgb(17,17,17)\",\"radialaxis\":{\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"rgb(17,17,17)\",\"gridcolor\":\"#506784\",\"gridwidth\":2,\"linecolor\":\"#506784\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#C8D4E3\"},\"yaxis\":{\"backgroundcolor\":\"rgb(17,17,17)\",\"gridcolor\":\"#506784\",\"gridwidth\":2,\"linecolor\":\"#506784\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#C8D4E3\"},\"zaxis\":{\"backgroundcolor\":\"rgb(17,17,17)\",\"gridcolor\":\"#506784\",\"gridwidth\":2,\"linecolor\":\"#506784\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#C8D4E3\"}},\"shapedefaults\":{\"line\":{\"color\":\"#f2f5fa\"}},\"sliderdefaults\":{\"bgcolor\":\"#C8D4E3\",\"bordercolor\":\"rgb(17,17,17)\",\"borderwidth\":1,\"tickwidth\":0},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"ticks\":\"\"},\"bgcolor\":\"rgb(17,17,17)\",\"caxis\":{\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"updatemenudefaults\":{\"bgcolor\":\"#506784\",\"borderwidth\":0},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#283442\",\"linecolor\":\"#506784\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#283442\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#283442\",\"linecolor\":\"#506784\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#283442\",\"zerolinewidth\":2}}},\"legend\":{\"title\":{\"text\":\"Chunk Types\"}},\"barmode\":\"group\",\"xaxis\":{\"title\":{\"text\":\"Chunk Index\"}},\"yaxis\":{\"title\":{\"text\":\"Chunk Length (Characters)\"}},\"title\":{\"text\":\"Comparison of Chunk Lengths: Semantic vs Recursive\"},\"hovermode\":\"x unified\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('b34e4a38-eb76-4bcf-96ef-f466d9eda084');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y5N6n9kELzQV"
      },
      "id": "y5N6n9kELzQV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nFfHS_fELzS9"
      },
      "id": "nFfHS_fELzS9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f66569f0",
      "metadata": {
        "id": "f66569f0",
        "outputId": "1b98f21b-1830-48dc-f48d-bdb54734b375"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Now we have 5 documents and the first one has 2086 tokens\n"
          ]
        }
      ],
      "source": [
        "num_docs = len(docs)\n",
        "\n",
        "num_tokens_first_doc = llm.get_num_tokens(docs[0].page_content)\n",
        "\n",
        "print (f\"Now we have {num_docs} documents and the first one has {num_tokens_first_doc} tokens\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b61d49f5",
      "metadata": {
        "id": "b61d49f5"
      },
      "source": [
        "Great, assuming that number of tokens is consistent in the other docs we should be good to go. Let's use LangChain's [load_summarize_chain](https://python.langchain.com/en/latest/use_cases/summarization.html) to do the `map_reducing` for us. We first need to initialize our chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3b08c54",
      "metadata": {
        "id": "b3b08c54"
      },
      "outputs": [],
      "source": [
        "summary_chain = load_summarize_chain(llm=llm, chain_type='map_reduce',\n",
        "#                                      verbose=True # Set verbose=True if you want to see the prompts being used\n",
        "                                    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78c45721",
      "metadata": {
        "id": "78c45721"
      },
      "source": [
        "Now actually run it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba73121e",
      "metadata": {
        "id": "ba73121e"
      },
      "outputs": [],
      "source": [
        "output = summary_chain.run(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "693acf9d",
      "metadata": {
        "id": "693acf9d",
        "outputId": "49407c7e-acb5-46d1-bf04-5e1408f7e322"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "' This article provides strategies for coming up with startup ideas on demand, such as looking in areas of expertise, talking to people about their needs, and looking for waves and gaps in the market. It also discusses the need for users to have sufficient activation energy to start using a product, and how this varies depending on the product. It looks at the difficulty of switching paths in life as one gets older, and how colleges can help students start startups. Finally, it looks at the importance of focusing on users rather than competitors, and how Steve Wozniak solved his own problems.'"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3bc3a71e",
      "metadata": {
        "id": "3bc3a71e"
      },
      "source": [
        "This summary is a great start, but I'm more of a bullet point person. I want to get my final output in bullet point form.\n",
        "\n",
        "In order to do this I'm going to use custom promopts (like we did above) to instruct the model on what I want.\n",
        "\n",
        "The map_prompt is going to stay the same (just showing it for clarity), but I'll edit the combine_prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7442e889",
      "metadata": {
        "id": "7442e889"
      },
      "outputs": [],
      "source": [
        "map_prompt = \"\"\"\n",
        "Write a concise summary of the following:\n",
        "\"{text}\"\n",
        "CONCISE SUMMARY:\n",
        "\"\"\"\n",
        "map_prompt_template = PromptTemplate(template=map_prompt, input_variables=[\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f42deb5",
      "metadata": {
        "id": "6f42deb5"
      },
      "outputs": [],
      "source": [
        "combine_prompt = \"\"\"\n",
        "Write a concise summary of the following text delimited by triple backquotes.\n",
        "Return your response in bullet points which covers the key points of the text.\n",
        "```{text}```\n",
        "BULLET POINT SUMMARY:\n",
        "\"\"\"\n",
        "combine_prompt_template = PromptTemplate(template=combine_prompt, input_variables=[\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2051818",
      "metadata": {
        "id": "e2051818"
      },
      "outputs": [],
      "source": [
        "summary_chain = load_summarize_chain(llm=llm,\n",
        "                                     chain_type='map_reduce',\n",
        "                                     map_prompt=map_prompt_template,\n",
        "                                     combine_prompt=combine_prompt_template,\n",
        "#                                      verbose=True\n",
        "                                    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c986230f",
      "metadata": {
        "id": "c986230f"
      },
      "outputs": [],
      "source": [
        "output = summary_chain.run(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e90a8582",
      "metadata": {
        "id": "e90a8582",
        "outputId": "e5b680f0-471e-4e72-803c-d57b509e821e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "- Y Combinator suggests that the best startup ideas come from looking for problems, preferably ones that the founders have themselves.\n",
            "- Good ideas should appeal to a small number of people who need it urgently.\n",
            "- To find startup ideas, one should look for things that seem to be missing and be prepared to question the status quo.\n",
            "- College students should use their college experience to prepare themselves for the future and build things with other students.\n",
            "- Tricks for coming up with startup ideas on demand include looking in areas of expertise, talking to people about their needs, and looking for waves and gaps in the market.\n",
            "- Sam Altman points out that taking the time to come up with an idea is a better strategy than most founders are willing to put in the time for.\n",
            "- Paul Buchheit suggests that trying to sell something bad can lead to better ideas.\n"
          ]
        }
      ],
      "source": [
        "print (output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59a6da87",
      "metadata": {
        "id": "59a6da87"
      },
      "source": [
        "## Level 4: Best Representation Vectors - Summarize an entire book\n",
        "\n",
        "In the above method we pass the entire document (all 9.5K tokens of it) to the LLM. But what if you have more tokens than that?\n",
        "\n",
        "What if you had a book you wanted to summarize? Let's load one up, we're going to load [Into Thin Air](https://www.amazon.com/Into-Thin-Air-Personal-Disaster/dp/0385494785) about the 1996 Everest Disaster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6ed1dcb",
      "metadata": {
        "id": "f6ed1dcb"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "# Load the book\n",
        "loader = PyPDFLoader(\"../data/IntoThinAirBook.pdf\")\n",
        "pages = loader.load()\n",
        "\n",
        "# Cut out the open and closing parts\n",
        "pages = pages[26:277]\n",
        "\n",
        "# Combine the pages, and replace the tabs with spaces\n",
        "text = \"\"\n",
        "\n",
        "for page in pages:\n",
        "    text += page.page_content\n",
        "\n",
        "text = text.replace('\\t', ' ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6db88712",
      "metadata": {
        "id": "6db88712",
        "outputId": "dbd74acb-d122-4785-ca72-9094ca60c8f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This book has 139472 tokens in it\n"
          ]
        }
      ],
      "source": [
        "num_tokens = llm.get_num_tokens(text)\n",
        "\n",
        "print (f\"This book has {num_tokens} tokens in it\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bedfe55",
      "metadata": {
        "id": "4bedfe55"
      },
      "source": [
        "Wow, that's over 100K tokens, even [GPT 32K](https://help.openai.com/en/articles/7127966-what-is-the-difference-between-the-gpt-4-models) wouldn't be able to handle that in one go. At [0.03 per 1K prompt tokens](https://help.openai.com/en/articles/7127956-how-much-does-gpt-4-cost), this would cost us $4.17 just for the prompt alone.\n",
        "\n",
        "So how do we do this without going through all the tokens? Pick random chunks? Pick equally spaced chunks?\n",
        "\n",
        "I kicked off a [twitter thread](https://twitter.com/GregKamradt/status/1653060004226924544) with a proposed solution to see if I was off base. I'm calling it the Best Representation Vectors method (not sure if a name already exists for it).\n",
        "\n",
        "**Goal:** Chunk your book then get embeddings of the chunks. Pick a subset of chunks which represent a wholistic but diverse view of the book. Or another way, is there a way to pick the top 10 passages that describe the book the best?\n",
        "\n",
        "Once we have our chunks that represent the book then we can summarize those chunks and hopefully get a pretty good summary.\n",
        "\n",
        "Keep in mind there are tools that would likely do this for you, and with token limits increasing this won't be a problem for long. But if you want to do it from scratch this might help.\n",
        "\n",
        "This is most definitely not the optimal answer, but it's my take on it for now! If the [clustering](https://scikit-learn.org/stable/modules/clustering.html) experts wanna help improve it that would be awesome.\n",
        "\n",
        "**The BRV Steps:**\n",
        "1. Load your book into a single text file\n",
        "2. Split your text into large-ish chunks\n",
        "3. Embed your chunks to get vectors\n",
        "4. Cluster the vectors to see which are similar to each other and likely talk about the same parts of the book\n",
        "5. Pick embeddings that represent the cluster the most (method: closest to each cluster centroid)\n",
        "6. Summarize the documents that these embeddings represent\n",
        "\n",
        "Another way to phrase this process, \"Which ~10 documents from this book represent most of the meaning? I want to build a summary off those.\"\n",
        "\n",
        "Note: There will be a bit of information loss, but show me a summary of a whole book that doesn't have information loss ;)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0e930b3",
      "metadata": {
        "id": "e0e930b3",
        "outputId": "2e8c75b8-e719-482c-9d45-f42b6c2c4e68"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/gregorykamradt/opt/anaconda3/lib/python3.9/site-packages/deeplake/util/check_latest_version.py:32: UserWarning: A newer version of deeplake (3.7.2) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Loaders\n",
        "from langchain.schema import Document\n",
        "\n",
        "# Splitters\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Model\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "# Embedding Support\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "\n",
        "# Summarizer we'll use for Map Reduce\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "\n",
        "# Data Science\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e5cdbd2",
      "metadata": {
        "id": "0e5cdbd2"
      },
      "source": [
        "I'm going to initialize two models, gpt-3.5 and gpt4. I'll use gpt 3.5 for the first set of summaries to reduce cost and then gpt4 for the final pass which should hopefully increase the quality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d3d233d",
      "metadata": {
        "id": "0d3d233d"
      },
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(separators=[\"\\n\\n\", \"\\n\", \"\\t\"], chunk_size=10000, chunk_overlap=3000)\n",
        "\n",
        "docs = text_splitter.create_documents([text])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1c2c701",
      "metadata": {
        "id": "c1c2c701",
        "outputId": "22977b42-b892-448a-becc-13819cf32f7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Now our book is split up into 78 documents\n"
          ]
        }
      ],
      "source": [
        "num_documents = len(docs)\n",
        "\n",
        "print (f\"Now our book is split up into {num_documents} documents\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d413b2d",
      "metadata": {
        "id": "8d413b2d"
      },
      "source": [
        "Let's get our embeddings of those 78 documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "898fc62c",
      "metadata": {
        "id": "898fc62c"
      },
      "outputs": [],
      "source": [
        "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
        "\n",
        "vectors = embeddings.embed_documents([x.page_content for x in docs])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "726f6dee",
      "metadata": {
        "id": "726f6dee"
      },
      "source": [
        "Now let's cluster our embeddings. There are a ton of clustering algorithms you can chose from. Please try a few out to see what works best for you!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55c8dce0",
      "metadata": {
        "id": "55c8dce0"
      },
      "outputs": [],
      "source": [
        "# Assuming 'embeddings' is a list or array of 1536-dimensional embeddings\n",
        "\n",
        "# Choose the number of clusters, this can be adjusted based on the book's content.\n",
        "# I played around and found ~10 was the best.\n",
        "# Usually if you have 10 passages from a book you can tell what it's about\n",
        "num_clusters = 11\n",
        "\n",
        "# Perform K-means clustering\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=42).fit(vectors)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "891d82cd",
      "metadata": {
        "id": "891d82cd"
      },
      "source": [
        "Here are the clusters that were found. It's interesting to see the progression of clusters throughout the book. This is expected because as the plot changes you'd expect different clusters to emerge due to different semantic meaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "218dce05",
      "metadata": {
        "id": "218dce05"
      },
      "outputs": [],
      "source": [
        "kmeans.labels_"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d9ccd8b",
      "metadata": {
        "id": "1d9ccd8b"
      },
      "source": [
        "This is sweet, but whenever you have a clustering exercise, it's hard *not* to graph them. Make sure you add colors.\n",
        "\n",
        "We also need to do dimensionality reduction to reduce the vectors from 1536 dimensions to 2 (this is sloppy data science but we are working towards the 80% solution)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95da2509",
      "metadata": {
        "id": "95da2509"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Taking out the warnings\n",
        "import warnings\n",
        "from warnings import simplefilter\n",
        "\n",
        "# Filter out FutureWarnings\n",
        "simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "# Perform t-SNE and reduce to 2 dimensions\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "reduced_data_tsne = tsne.fit_transform(vectors)\n",
        "\n",
        "# Plot the reduced data\n",
        "plt.scatter(reduced_data_tsne[:, 0], reduced_data_tsne[:, 1], c=kmeans.labels_)\n",
        "plt.xlabel('Dimension 1')\n",
        "plt.ylabel('Dimension 2')\n",
        "plt.title('Book Embeddings Clustered')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6aa4314b",
      "metadata": {
        "id": "6aa4314b"
      },
      "source": [
        "Awesome, not perfect, but pretty good directionally. Now we need to get the vectors which are closest to the cluster centroids (the center).\n",
        "\n",
        "The function below is a quick way to do that (w/ help from ChatGPT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55f2f664",
      "metadata": {
        "id": "55f2f664"
      },
      "outputs": [],
      "source": [
        "# Find the closest embeddings to the centroids\n",
        "\n",
        "# Create an empty list that will hold your closest points\n",
        "closest_indices = []\n",
        "\n",
        "# Loop through the number of clusters you have\n",
        "for i in range(num_clusters):\n",
        "\n",
        "    # Get the list of distances from that particular cluster center\n",
        "    distances = np.linalg.norm(vectors - kmeans.cluster_centers_[i], axis=1)\n",
        "\n",
        "    # Find the list position of the closest one (using argmin to find the smallest distance)\n",
        "    closest_index = np.argmin(distances)\n",
        "\n",
        "    # Append that position to your closest indices list\n",
        "    closest_indices.append(closest_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5a44596",
      "metadata": {
        "id": "f5a44596"
      },
      "source": [
        "Now sort them (so the chunks are processed in order)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "679297ac",
      "metadata": {
        "id": "679297ac"
      },
      "outputs": [],
      "source": [
        "selected_indices = sorted(closest_indices)\n",
        "selected_indices"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "171c2b42",
      "metadata": {
        "id": "171c2b42"
      },
      "source": [
        "It's intersting to see which chunks pop up at most descriptive. How does your distribution look?\n",
        "\n",
        "Let's create our custom prompts. I'm going to use gpt4 (which has a bigger token limit) for the combine step so I'm asking for long summaries in the map step to reduce the information loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83bc55d8",
      "metadata": {
        "id": "83bc55d8"
      },
      "outputs": [],
      "source": [
        "llm3 = ChatOpenAI(temperature=0,\n",
        "                 openai_api_key=openai_api_key,\n",
        "                 max_tokens=1000,\n",
        "                 model='gpt-3.5-turbo'\n",
        "                )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5552e6d3",
      "metadata": {
        "id": "5552e6d3"
      },
      "outputs": [],
      "source": [
        "map_prompt = \"\"\"\n",
        "You will be given a single passage of a book. This section will be enclosed in triple backticks (```)\n",
        "Your goal is to give a summary of this section so that a reader will have a full understanding of what happened.\n",
        "Your response should be at least three paragraphs and fully encompass what was said in the passage.\n",
        "\n",
        "```{text}```\n",
        "FULL SUMMARY:\n",
        "\"\"\"\n",
        "map_prompt_template = PromptTemplate(template=map_prompt, input_variables=[\"text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b568b94a",
      "metadata": {
        "id": "b568b94a"
      },
      "source": [
        "I kept getting a timeout errors so I'm actually going to do this map reduce manually"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb5eb0d9",
      "metadata": {
        "id": "bb5eb0d9"
      },
      "outputs": [],
      "source": [
        "map_chain = load_summarize_chain(llm=llm3,\n",
        "                             chain_type=\"stuff\",\n",
        "                             prompt=map_prompt_template)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4e1ec38",
      "metadata": {
        "id": "e4e1ec38"
      },
      "source": [
        "Then go get your docs which the top vectors represented."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "781a686c",
      "metadata": {
        "id": "781a686c"
      },
      "outputs": [],
      "source": [
        "selected_docs = [docs[doc] for doc in selected_indices]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eca5fc42",
      "metadata": {
        "id": "eca5fc42"
      },
      "source": [
        "Let's loop through our selected docs and get a good summary for each chunk. We'll store the summary in a list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e2220ab",
      "metadata": {
        "id": "7e2220ab"
      },
      "outputs": [],
      "source": [
        "# Make an empty list to hold your summaries\n",
        "summary_list = []\n",
        "\n",
        "# Loop through a range of the lenght of your selected docs\n",
        "for i, doc in enumerate(selected_docs):\n",
        "\n",
        "    # Go get a summary of the chunk\n",
        "    chunk_summary = map_chain.run([doc])\n",
        "\n",
        "    # Append that summary to your list\n",
        "    summary_list.append(chunk_summary)\n",
        "\n",
        "    print (f\"Summary #{i} (chunk #{selected_indices[i]}) - Preview: {chunk_summary[:250]} \\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7fae1a72",
      "metadata": {
        "id": "7fae1a72"
      },
      "source": [
        "Great, now that we have our list of summaries, let's get a summary of the summaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4083431d",
      "metadata": {
        "id": "4083431d"
      },
      "outputs": [],
      "source": [
        "summaries = \"\\n\".join(summary_list)\n",
        "\n",
        "# Convert it back to a document\n",
        "summaries = Document(page_content=summaries)\n",
        "\n",
        "print (f\"Your total summary has {llm.get_num_tokens(summaries.page_content)} tokens\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d2e71c5",
      "metadata": {
        "id": "1d2e71c5"
      },
      "outputs": [],
      "source": [
        "llm4 = ChatOpenAI(temperature=0,\n",
        "                 openai_api_key=openai_api_key,\n",
        "                 max_tokens=3000,\n",
        "                 model='gpt-4',\n",
        "                 request_timeout=120\n",
        "                )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b98e7321",
      "metadata": {
        "id": "b98e7321"
      },
      "outputs": [],
      "source": [
        "combine_prompt = \"\"\"\n",
        "You will be given a series of summaries from a book. The summaries will be enclosed in triple backticks (```)\n",
        "Your goal is to give a verbose summary of what happened in the story.\n",
        "The reader should be able to grasp what happened in the book.\n",
        "\n",
        "```{text}```\n",
        "VERBOSE SUMMARY:\n",
        "\"\"\"\n",
        "combine_prompt_template = PromptTemplate(template=combine_prompt, input_variables=[\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2930c26e",
      "metadata": {
        "id": "2930c26e"
      },
      "outputs": [],
      "source": [
        "reduce_chain = load_summarize_chain(llm=llm4,\n",
        "                             chain_type=\"stuff\",\n",
        "                             prompt=combine_prompt_template,\n",
        "#                              verbose=True # Set this to true if you want to see the inner workings\n",
        "                                   )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9958142",
      "metadata": {
        "id": "e9958142"
      },
      "source": [
        "Run! Note this will take a while"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6eda913",
      "metadata": {
        "id": "c6eda913"
      },
      "outputs": [],
      "source": [
        "output = reduce_chain.run([summaries])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "915628a1",
      "metadata": {
        "id": "915628a1"
      },
      "outputs": [],
      "source": [
        "print (output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27a1db73",
      "metadata": {
        "id": "27a1db73"
      },
      "source": [
        "Wow that was a long process, but you get the gist, hopefully we'll see some library abstractions in the coming months that do this automatically for us! Let me know what you think on [Twitter](https://twitter.com/GregKamradt)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b4ae429",
      "metadata": {
        "id": "5b4ae429"
      },
      "source": [
        "## Level 5: Agents - Summarize an unknown amount of text\n",
        "\n",
        "What if you have an unknown amount of text you need to summarize? This may be a verticalize use case (like law or medical) where more research is required as you uncover the first pieces of information.\n",
        "\n",
        "We're going to use agents below, this is still a very actively developed area and should be handled with care. Future agents will be able to handle a lot more complicated tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd384fd2",
      "metadata": {
        "id": "fd384fd2"
      },
      "outputs": [],
      "source": [
        "from langchain import OpenAI\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.agents import initialize_agent, Tool\n",
        "from langchain.utilities import WikipediaAPIWrapper\n",
        "\n",
        "llm = ChatOpenAI(temperature=0, model_name='gpt-4', openai_api_key=openai_api_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6456e70a",
      "metadata": {
        "id": "6456e70a"
      },
      "source": [
        "We're going to use the Wiki search tool and research multiple topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b93660c",
      "metadata": {
        "id": "6b93660c"
      },
      "outputs": [],
      "source": [
        "wikipedia = WikipediaAPIWrapper()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7d26c52",
      "metadata": {
        "id": "b7d26c52"
      },
      "source": [
        "Let's define our toolkit, in this case it's just one tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aaf60c86",
      "metadata": {
        "id": "aaf60c86"
      },
      "outputs": [],
      "source": [
        "tools = [\n",
        "    Tool(\n",
        "        name=\"Wikipedia\",\n",
        "        func=wikipedia.run,\n",
        "        description=\"Useful for when you need to get information from wikipedia about a single topic\"\n",
        "    ),\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d0f07a5",
      "metadata": {
        "id": "9d0f07a5"
      },
      "source": [
        "Init our agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "baff1fce",
      "metadata": {
        "id": "baff1fce"
      },
      "outputs": [],
      "source": [
        "agent_executor = initialize_agent(tools, llm, agent='zero-shot-react-description', verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06d17afb",
      "metadata": {
        "id": "06d17afb"
      },
      "source": [
        "Then let's ask a question that will need multiple documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbd6e437",
      "metadata": {
        "scrolled": false,
        "id": "dbd6e437"
      },
      "outputs": [],
      "source": [
        "output = agent_executor.run(\"Can you please provide a quick summary of Napoleon Bonaparte? \\\n",
        "                          Then do a separate search and tell me what the commonalities are with Serena Williams\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e5aeeee",
      "metadata": {
        "id": "4e5aeeee"
      },
      "outputs": [],
      "source": [
        "print (output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f48735b9",
      "metadata": {
        "id": "f48735b9"
      },
      "source": [
        "Awesome, good luck summarizing!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}