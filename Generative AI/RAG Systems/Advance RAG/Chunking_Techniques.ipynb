{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "RigieEz5MLbL"
      },
      "outputs": [],
      "source": [
        "# !pip install langchain\n",
        "# !pip install langchain-community\n",
        "# !pip install langchain-experimental\n",
        "# !pip install PyPDF2\n",
        "# !pip install langchain_groq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
        "from rich import print\n",
        "import PyPDF2\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "h-9f1EwNMofe"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedding Model\n",
        "embedding_model = HuggingFaceInferenceAPIEmbeddings(api_key =userdata.get('HF_TOKEN'),model_name=\"BAAI/bge-base-en-v1.5\")"
      ],
      "metadata": {
        "id": "CPGdm--9SpZH"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining the text for reference"
      ],
      "metadata": {
        "id": "tZyC2CyGNQ_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_content(url:str):\n",
        "\n",
        "  # Send a GET request to the webpage\n",
        "  response = requests.get(url)\n",
        "\n",
        "  # Check if the request was successful\n",
        "  if response.status_code == 200:\n",
        "      # Parse the HTML content using BeautifulSoup\n",
        "      soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "      # Extract the main content (text)\n",
        "      # This site has the main text within <font> tags\n",
        "      content = soup.find_all(\"font\")\n",
        "\n",
        "      # Combine and clean the extracted text\n",
        "      full_text = \"\\n\".join([element.get_text() for element in content])\n",
        "\n",
        "      return full_text\n",
        "  else:\n",
        "      print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n",
        "\n",
        "url = \"https://paulgraham.com/persistence.html\"\n",
        "# essay = get_content(url)\n",
        "# print(essay)"
      ],
      "metadata": {
        "id": "ustSQiH_NSfl"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above code is just for your reference that if you want to explore the online blogs then you can simply pass the url and it will be automatically extracted."
      ],
      "metadata": {
        "id": "YUtwTnBGOwWc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Open the PDF file in binary mode\n",
        "with open(\"/content/RAG.pdf\", \"rb\") as file:\n",
        "    reader = PyPDF2.PdfReader(file)\n",
        "    content = \"\"\n",
        "\n",
        "    # Calculate the number of pages to read (excluding the last 3 pages)\n",
        "    total_pages = len(reader.pages)\n",
        "    pages_to_read = total_pages - 5\n",
        "\n",
        "    # Extract text from the pages, excluding the last 3\n",
        "    for page_num in range(pages_to_read):\n",
        "        content += reader.pages[page_num].extract_text()"
      ],
      "metadata": {
        "id": "iohpsParNtRW"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Technqiue 1 : Character Text Splitter"
      ],
      "metadata": {
        "id": "4f9wQa1xNzMc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = CharacterTextSplitter(chunk_size = 1000, chunk_overlap=200, separator='', strip_whitespace=True)\n",
        "docs_technique1 = text_splitter.create_documents([content])"
      ],
      "metadata": {
        "id": "s8jPnIg0NurA"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Total documents created:\",len(docs_technique1))\n",
        "print(docs_technique1[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "LYi3fz7kQ81C",
        "outputId": "ec55d3ba-de46-45fb-957f-2e8f90f51248"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Total documents created: \u001b[1;36m92\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Total documents created: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">92</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
              "    \u001b[33mpage_content\u001b[0m=\u001b[32m'particularly for knowledge-intensive tasks, and allows\\nfor continuous knowledge updates and \u001b[0m\n",
              "\u001b[32mintegration of domain-\\nspecific information. RAG synergistically merges LLMs’ intrin-\\nsic knowledge with the \u001b[0m\n",
              "\u001b[32mvast, dynamic repositories of external\\ndatabases. This comprehensive review paper offers a detailed\\nexamination \u001b[0m\n",
              "\u001b[32mof the progression of RAG paradigms, encompassing\\nthe Naive RAG, the Advanced RAG, and the Modular RAG.\\nIt \u001b[0m\n",
              "\u001b[32mmeticulously scrutinizes the tripartite foundation of RAG\\nframeworks, which includes the retrieval, the generation\u001b[0m\n",
              "\u001b[32mand the\\naugmentation techniques. The paper highlights the state-of-the-\\nart technologies embedded in each of \u001b[0m\n",
              "\u001b[32mthese critical components,\\nproviding a profound understanding of the advancements in RAG\\nsystems. Furthermore, \u001b[0m\n",
              "\u001b[32mthis paper introduces up-to-date evalua-\\ntion framework and benchmark. At the end, this article delineates\\nthe \u001b[0m\n",
              "\u001b[32mchallenges currently faced and points out prospective avenues\\nfor research and development1.\\nIndex Terms —Large \u001b[0m\n",
              "\u001b[32mlanguage model, retri'\u001b[0m\n",
              "\u001b[1m)\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'particularly for knowledge-intensive tasks, and allows\\nfor continuous knowledge updates and </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">integration of domain-\\nspecific information. RAG synergistically merges LLMs’ intrin-\\nsic knowledge with the </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">vast, dynamic repositories of external\\ndatabases. This comprehensive review paper offers a detailed\\nexamination </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">of the progression of RAG paradigms, encompassing\\nthe Naive RAG, the Advanced RAG, and the Modular RAG.\\nIt </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">meticulously scrutinizes the tripartite foundation of RAG\\nframeworks, which includes the retrieval, the generation</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">and the\\naugmentation techniques. The paper highlights the state-of-the-\\nart technologies embedded in each of </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">these critical components,\\nproviding a profound understanding of the advancements in RAG\\nsystems. Furthermore, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">this paper introduces up-to-date evalua-\\ntion framework and benchmark. At the end, this article delineates\\nthe </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">challenges currently faced and points out prospective avenues\\nfor research and development1.\\nIndex Terms —Large </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">language model, retri'</span>\n",
              "<span style=\"font-weight: bold\">)</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Technique 2 : Recursive text splitter"
      ],
      "metadata": {
        "id": "iS0GeQMiN2Pr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter_recursive = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, separators=[\"\\n\\n\", \"\\n\", \" \", \"\"])\n",
        "docs_technique2 = text_splitter_recursive.create_documents([content])"
      ],
      "metadata": {
        "id": "WXZAcL4eN4m8"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Total documents created:\",len(docs_technique2))\n",
        "print(docs_technique2[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "YUJccLiER_hD",
        "outputId": "8f5e6bd0-fd7a-4364-d9a0-be8fc6fbe5e0"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Total documents created: \u001b[1;36m93\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Total documents created: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">93</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
              "    \u001b[33mpage_content\u001b[0m=\u001b[32m'generation, particularly for knowledge-intensive tasks, and allows\\nfor continuous knowledge \u001b[0m\n",
              "\u001b[32mupdates and integration of domain-\\nspecific information. RAG synergistically merges LLMs’ intrin-\\nsic knowledge \u001b[0m\n",
              "\u001b[32mwith the vast, dynamic repositories of external\\ndatabases. This comprehensive review paper offers a \u001b[0m\n",
              "\u001b[32mdetailed\\nexamination of the progression of RAG paradigms, encompassing\\nthe Naive RAG, the Advanced RAG, and the \u001b[0m\n",
              "\u001b[32mModular RAG.\\nIt meticulously scrutinizes the tripartite foundation of RAG\\nframeworks, which includes the \u001b[0m\n",
              "\u001b[32mretrieval, the generation and the\\naugmentation techniques. The paper highlights the state-of-the-\\nart \u001b[0m\n",
              "\u001b[32mtechnologies embedded in each of these critical components,\\nproviding a profound understanding of the advancements\u001b[0m\n",
              "\u001b[32min RAG\\nsystems. Furthermore, this paper introduces up-to-date evalua-\\ntion framework and benchmark. At the end, \u001b[0m\n",
              "\u001b[32mthis article delineates\\nthe challenges currently faced and points out prospective avenues\\nfor research and \u001b[0m\n",
              "\u001b[32mdevelopment1.'\u001b[0m\n",
              "\u001b[1m)\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'generation, particularly for knowledge-intensive tasks, and allows\\nfor continuous knowledge </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">updates and integration of domain-\\nspecific information. RAG synergistically merges LLMs’ intrin-\\nsic knowledge </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">with the vast, dynamic repositories of external\\ndatabases. This comprehensive review paper offers a </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">detailed\\nexamination of the progression of RAG paradigms, encompassing\\nthe Naive RAG, the Advanced RAG, and the </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Modular RAG.\\nIt meticulously scrutinizes the tripartite foundation of RAG\\nframeworks, which includes the </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">retrieval, the generation and the\\naugmentation techniques. The paper highlights the state-of-the-\\nart </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">technologies embedded in each of these critical components,\\nproviding a profound understanding of the advancements</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">in RAG\\nsystems. Furthermore, this paper introduces up-to-date evalua-\\ntion framework and benchmark. At the end, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">this article delineates\\nthe challenges currently faced and points out prospective avenues\\nfor research and </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">development1.'</span>\n",
              "<span style=\"font-weight: bold\">)</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Semantic Splitting"
      ],
      "metadata": {
        "id": "5kUeXsCmN5KS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "text_splitter_semantic = SemanticChunker(embedding_model)\n",
        "docs_technique3 = text_splitter_semantic.create_documents([content])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwPur7oRN6pT",
        "outputId": "46eb2d23-0cc9-474e-cc16-8b31ac1ea745"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 364 ms, sys: 73.5 ms, total: 438 ms\n",
            "Wall time: 1.43 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Total documents created:\",len(docs_technique3))\n",
        "print(docs_technique3[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 945
        },
        "id": "BTD6Dh40S7UM",
        "outputId": "865d429b-b44c-4dcd-ecaf-9d66df920fbd"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Total documents created: \u001b[1;36m26\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Total documents created: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
              "    \u001b[33mpage_content\u001b[0m=\u001b[32m'At the end, this article delineates\\nthe challenges currently faced and points out prospective \u001b[0m\n",
              "\u001b[32mavenues\\nfor research and development1. Index Terms —Large language model, retrieval-augmented gen-\\neration, \u001b[0m\n",
              "\u001b[32mnatural language processing, information retrieval\\nI. I NTRODUCTION\\nLARGE language models \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m have achieved \u001b[0m\n",
              "\u001b[32mremark-\\nable success, though they still face significant limitations,\\nespecially in domain-specific or \u001b[0m\n",
              "\u001b[32mknowledge-intensive tasks \u001b[0m\u001b[32m[\u001b[0m\u001b[32m1\u001b[0m\u001b[32m]\u001b[0m\u001b[32m,\\nnotably producing “hallucinations” \u001b[0m\u001b[32m[\u001b[0m\u001b[32m2\u001b[0m\u001b[32m]\u001b[0m\u001b[32m when handling queries\\nbeyond their training\u001b[0m\n",
              "\u001b[32mdata or requiring current information. To\\novercome challenges, Retrieval-Augmented Generation \u001b[0m\u001b[32m(\u001b[0m\u001b[32mRAG\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nenhances LLMs\u001b[0m\n",
              "\u001b[32mby retrieving relevant document chunks from\\nexternal knowledge base through semantic similarity calcu-\\nlation. By\u001b[0m\n",
              "\u001b[32mreferencing external knowledge, RAG effectively\\nreduces the problem of generating factually incorrect content. Its\u001b[0m\n",
              "\u001b[32mintegration into LLMs has resulted in widespread adoption,\\nestablishing RAG as a key technology in advancing \u001b[0m\n",
              "\u001b[32mchatbots\\nand enhancing the suitability of LLMs for real-world applica-\\ntions. RAG technology has rapidly \u001b[0m\n",
              "\u001b[32mdeveloped in recent years, and\\nthe technology tree summarizing related research is shown\\nCorresponding \u001b[0m\n",
              "\u001b[32mAuthor.Email:haofen.wang@tongji.edu.cn\\n1Resources are available at https://github.com/Tongji-KGLLM/\\nRAG-Surveyin \u001b[0m\n",
              "\u001b[32mFigure 1. The development trajectory of RAG in the era\\nof large models exhibits several distinct stage \u001b[0m\n",
              "\u001b[32mcharacteristics. Initially, RAG’s inception coincided with the rise of the\\nTransformer architecture, focusing on \u001b[0m\n",
              "\u001b[32menhancing language\\nmodels by incorporating additional knowledge through Pre-\\nTraining Models \u001b[0m\u001b[32m(\u001b[0m\u001b[32mPTM\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. This early \u001b[0m\n",
              "\u001b[32mstage was characterized\\nby foundational work aimed at refining pre-training techniques\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m3\u001b[0m\u001b[32m]\u001b[0m\u001b[32m–\u001b[0m\u001b[32m[\u001b[0m\u001b[32m5\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.The subsequent \u001b[0m\n",
              "\u001b[32marrival of ChatGPT \u001b[0m\u001b[32m[\u001b[0m\u001b[32m6\u001b[0m\u001b[32m]\u001b[0m\u001b[32m marked a\\npivotal moment, with LLM demonstrating powerful in context\\nlearning \u001b[0m\u001b[32m(\u001b[0m\u001b[32mICL\u001b[0m\u001b[32m)\u001b[0m\u001b[32m \u001b[0m\n",
              "\u001b[32mcapabilities. RAG research shifted towards\\nproviding better information for LLMs to answer more com-\\nplex and \u001b[0m\n",
              "\u001b[32mknowledge-intensive tasks during the inference stage,\\nleading to rapid development in RAG studies. As \u001b[0m\n",
              "\u001b[32mresearch\\nprogressed, the enhancement of RAG was no longer limited\\nto the inference stage but began to incorporate\u001b[0m\n",
              "\u001b[32mmore with LLM\\nfine-tuning techniques. The burgeoning field of RAG has experienced swift growth,\\nyet it has not \u001b[0m\n",
              "\u001b[32mbeen accompanied by a systematic synthesis that\\ncould clarify its broader trajectory. This survey endeavors \u001b[0m\n",
              "\u001b[32mto\\nfill this gap by mapping out the RAG process and charting\\nits evolution and anticipated future paths, with a \u001b[0m\n",
              "\u001b[32mfocus on the\\nintegration of RAG within LLMs. This paper considers both\\ntechnical paradigms and research methods, \u001b[0m\n",
              "\u001b[32msummarizing three\\nmain research paradigms from over 100 RAG studies, and\\nanalyzing key technologies in the core \u001b[0m\n",
              "\u001b[32mstages of “Retrieval,”\\n“Generation,” and “Augmentation.” On the other hand, current\\nresearch tends to focus more \u001b[0m\n",
              "\u001b[32mon methods, lacking analysis and\\nsummarization of how to evaluate RAG. This paper compre-\\nhensively reviews the \u001b[0m\n",
              "\u001b[32mdownstream tasks, datasets, benchmarks,\\nand evaluation methods applicable to RAG. Overall, this\\npaper sets out to\u001b[0m\n",
              "\u001b[32mmeticulously compile and categorize the\\nfoundational technical concepts, historical progression, and\\nthe spectrum\u001b[0m\n",
              "\u001b[32mof RAG methodologies and applications that\\nhave emerged post-LLMs. It is designed to equip readers \u001b[0m\n",
              "\u001b[32mand\\nprofessionals with a detailed and structured understanding of\\nboth large models and RAG. It aims to \u001b[0m\n",
              "\u001b[32milluminate the evolution\\nof retrieval augmentation techniques, assess the strengths and\\nweaknesses of various \u001b[0m\n",
              "\u001b[32mapproaches in their respective contexts,\\nand speculate on upcoming trends and innovations. Our contributions are \u001b[0m\n",
              "\u001b[32mas follows:\\n•In this survey, we present a thorough and systematic\\nreview of the state-of-the-art RAG methods, \u001b[0m\n",
              "\u001b[32mdelineating\\nits evolution through paradigms including naive RAG,arXiv:2312.10997v5  \u001b[0m\u001b[32m[\u001b[0m\u001b[32mcs.CL\u001b[0m\u001b[32m]\u001b[0m\u001b[32m  27 Mar 20242\\nFig. 1.\u001b[0m\n",
              "\u001b[32mTechnology tree of RAG research. The stages of involving RAG mainly include pre-training, fine-tuning, and \u001b[0m\n",
              "\u001b[32minference. With the emergence of LLMs,\\nresearch on RAG initially focused on leveraging the powerful in context \u001b[0m\n",
              "\u001b[32mlearning abilities of LLMs, primarily concentrating on the inference stage. Subsequent\\nresearch has delved deeper,\u001b[0m\n",
              "\u001b[32mgradually integrating more with the fine-tuning of LLMs. Researchers have also been exploring ways to enhance \u001b[0m\n",
              "\u001b[32mlanguage models\\nin the pre-training stage through retrieval-augmented techniques. advanced RAG, and modular RAG. \u001b[0m\n",
              "\u001b[32mThis review contex-\\ntualizes the broader scope of RAG research within the\\nlandscape of LLMs. •We identify and \u001b[0m\n",
              "\u001b[32mdiscuss the central technologies integral\\nto the RAG process, specifically focusing on the aspects\\nof \u001b[0m\n",
              "\u001b[32m“Retrieval”, “Generation” and “Augmentation”, and\\ndelve into their synergies, elucidating how these com-\\nponents \u001b[0m\n",
              "\u001b[32mintricately collaborate to form a cohesive and\\neffective RAG framework. •We have summarized the current assessment\u001b[0m\n",
              "\u001b[32mmethods of\\nRAG, covering 26 tasks, nearly 50 datasets, outlining\\nthe evaluation objectives and metrics, as well \u001b[0m\n",
              "\u001b[32mas the\\ncurrent evaluation benchmarks and tools. Additionally,\\nwe anticipate future directions for RAG, \u001b[0m\n",
              "\u001b[32memphasizing\\npotential enhancements to tackle current challenges. The paper unfolds as follows: Section II \u001b[0m\n",
              "\u001b[32mintroduces the\\nmain concept and current paradigms of RAG. The following\\nthree sections explore core \u001b[0m\n",
              "\u001b[32mcomponents—“Retrieval”, “Gen-\\neration” and “Augmentation”, respectively. Section III focuses\\non optimization \u001b[0m\n",
              "\u001b[32mmethods in retrieval,including indexing, query\\nand embedding optimization. Section IV concentrates on \u001b[0m\n",
              "\u001b[32mpost-\\nretrieval process and LLM fine-tuning in generation. Section V\\nanalyzes the three augmentation processes. \u001b[0m\n",
              "\u001b[32mSection VI focuses\\non RAG’s downstream tasks and evaluation system. Sec-\\ntion VII mainly discusses the challenges\u001b[0m\n",
              "\u001b[32mthat RAG currentlyfaces and its future development directions. At last, the paper\\nconcludes in Section VIII.'\u001b[0m\n",
              "\u001b[1m)\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'At the end, this article delineates\\nthe challenges currently faced and points out prospective </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">avenues\\nfor research and development1. Index Terms —Large language model, retrieval-augmented gen-\\neration, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">natural language processing, information retrieval\\nI. I NTRODUCTION\\nLARGE language models (LLMs) have achieved </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">remark-\\nable success, though they still face significant limitations,\\nespecially in domain-specific or </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">knowledge-intensive tasks [1],\\nnotably producing “hallucinations” [2] when handling queries\\nbeyond their training</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">data or requiring current information. To\\novercome challenges, Retrieval-Augmented Generation (RAG)\\nenhances LLMs</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">by retrieving relevant document chunks from\\nexternal knowledge base through semantic similarity calcu-\\nlation. By</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">referencing external knowledge, RAG effectively\\nreduces the problem of generating factually incorrect content. Its</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">integration into LLMs has resulted in widespread adoption,\\nestablishing RAG as a key technology in advancing </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">chatbots\\nand enhancing the suitability of LLMs for real-world applica-\\ntions. RAG technology has rapidly </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">developed in recent years, and\\nthe technology tree summarizing related research is shown\\nCorresponding </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Author.Email:haofen.wang@tongji.edu.cn\\n1Resources are available at https://github.com/Tongji-KGLLM/\\nRAG-Surveyin </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Figure 1. The development trajectory of RAG in the era\\nof large models exhibits several distinct stage </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">characteristics. Initially, RAG’s inception coincided with the rise of the\\nTransformer architecture, focusing on </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">enhancing language\\nmodels by incorporating additional knowledge through Pre-\\nTraining Models (PTM). This early </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">stage was characterized\\nby foundational work aimed at refining pre-training techniques\\n[3]–[5].The subsequent </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">arrival of ChatGPT [6] marked a\\npivotal moment, with LLM demonstrating powerful in context\\nlearning (ICL) </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">capabilities. RAG research shifted towards\\nproviding better information for LLMs to answer more com-\\nplex and </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">knowledge-intensive tasks during the inference stage,\\nleading to rapid development in RAG studies. As </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">research\\nprogressed, the enhancement of RAG was no longer limited\\nto the inference stage but began to incorporate</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">more with LLM\\nfine-tuning techniques. The burgeoning field of RAG has experienced swift growth,\\nyet it has not </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">been accompanied by a systematic synthesis that\\ncould clarify its broader trajectory. This survey endeavors </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">to\\nfill this gap by mapping out the RAG process and charting\\nits evolution and anticipated future paths, with a </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">focus on the\\nintegration of RAG within LLMs. This paper considers both\\ntechnical paradigms and research methods, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">summarizing three\\nmain research paradigms from over 100 RAG studies, and\\nanalyzing key technologies in the core </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">stages of “Retrieval,”\\n“Generation,” and “Augmentation.” On the other hand, current\\nresearch tends to focus more </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">on methods, lacking analysis and\\nsummarization of how to evaluate RAG. This paper compre-\\nhensively reviews the </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">downstream tasks, datasets, benchmarks,\\nand evaluation methods applicable to RAG. Overall, this\\npaper sets out to</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">meticulously compile and categorize the\\nfoundational technical concepts, historical progression, and\\nthe spectrum</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">of RAG methodologies and applications that\\nhave emerged post-LLMs. It is designed to equip readers </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">and\\nprofessionals with a detailed and structured understanding of\\nboth large models and RAG. It aims to </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">illuminate the evolution\\nof retrieval augmentation techniques, assess the strengths and\\nweaknesses of various </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">approaches in their respective contexts,\\nand speculate on upcoming trends and innovations. Our contributions are </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">as follows:\\n•In this survey, we present a thorough and systematic\\nreview of the state-of-the-art RAG methods, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">delineating\\nits evolution through paradigms including naive RAG,arXiv:2312.10997v5  [cs.CL]  27 Mar 20242\\nFig. 1.</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Technology tree of RAG research. The stages of involving RAG mainly include pre-training, fine-tuning, and </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">inference. With the emergence of LLMs,\\nresearch on RAG initially focused on leveraging the powerful in context </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">learning abilities of LLMs, primarily concentrating on the inference stage. Subsequent\\nresearch has delved deeper,</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">gradually integrating more with the fine-tuning of LLMs. Researchers have also been exploring ways to enhance </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">language models\\nin the pre-training stage through retrieval-augmented techniques. advanced RAG, and modular RAG. </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">This review contex-\\ntualizes the broader scope of RAG research within the\\nlandscape of LLMs. •We identify and </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">discuss the central technologies integral\\nto the RAG process, specifically focusing on the aspects\\nof </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">“Retrieval”, “Generation” and “Augmentation”, and\\ndelve into their synergies, elucidating how these com-\\nponents </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">intricately collaborate to form a cohesive and\\neffective RAG framework. •We have summarized the current assessment</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">methods of\\nRAG, covering 26 tasks, nearly 50 datasets, outlining\\nthe evaluation objectives and metrics, as well </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">as the\\ncurrent evaluation benchmarks and tools. Additionally,\\nwe anticipate future directions for RAG, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">emphasizing\\npotential enhancements to tackle current challenges. The paper unfolds as follows: Section II </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">introduces the\\nmain concept and current paradigms of RAG. The following\\nthree sections explore core </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">components—“Retrieval”, “Gen-\\neration” and “Augmentation”, respectively. Section III focuses\\non optimization </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">methods in retrieval,including indexing, query\\nand embedding optimization. Section IV concentrates on </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">post-\\nretrieval process and LLM fine-tuning in generation. Section V\\nanalyzes the three augmentation processes. </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Section VI focuses\\non RAG’s downstream tasks and evaluation system. Sec-\\ntion VII mainly discusses the challenges</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">that RAG currentlyfaces and its future development directions. At last, the paper\\nconcludes in Section VIII.'</span>\n",
              "<span style=\"font-weight: bold\">)</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agentic Spliting"
      ],
      "metadata": {
        "id": "AB_2oLDnN6-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers.openai_tools import JsonOutputToolsParser\n",
        "from langchain_community.chat_models import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "from langchain.chains import create_extraction_chain\n",
        "from typing import Optional, List\n",
        "from langchain.chains import create_extraction_chain_pydantic\n",
        "from langchain_core.pydantic_v1 import BaseModel\n",
        "from langchain import hub\n",
        "from langchain_groq import ChatGroq\n",
        "import os"
      ],
      "metadata": {
        "id": "S-QY6Kx8N8FK"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the LLM model\n",
        "os.environ[\"GROQ_API_KEY\"] = userdata.get('GROQ_API_KEY')\n",
        "model = ChatGroq(model=\"llama3-8b-8192\")"
      ],
      "metadata": {
        "id": "oADi3wMaV0iv"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pulling out propositions is done via a well-crafted prompt. Let's pull it from LangHub, LangChain's home for prompts.\n",
        "obj = hub.pull(\"wfh/proposal-indexing\",api_key = userdata.get('LANGSMITH'))"
      ],
      "metadata": {
        "id": "r5aebASlVHbG"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output from a runnable is a json-esque structure in a string. We need to pull the sentences out. I found that LangChain's example extraction was giving me a hard time so I'm doing it manually with a pydantic data class. There is definitely room to improve this.\n",
        "\n",
        "Create your class then put it in an extraction chain."
      ],
      "metadata": {
        "id": "AKntvINuWU29"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use it in a runnable\n",
        "runnable = obj | model\n",
        "\n",
        "# Pydantic data class\n",
        "class Sentences(BaseModel):\n",
        "    sentences: List[str]\n",
        "\n",
        "# Extraction\n",
        "extraction_chain = create_extraction_chain_pydantic(pydantic_schema=Sentences, llm=model)"
      ],
      "metadata": {
        "id": "ytAANeamVgwn"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_propositions(text):\n",
        "    runnable_output = runnable.invoke({\n",
        "    \t\"input\": text\n",
        "    }).content\n",
        "\n",
        "    propositions = extraction_chain.run(runnable_output)[0].sentences\n",
        "    return propositions"
      ],
      "metadata": {
        "id": "vM-tE02WWXbz"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_content = docs_technique2[2].page_content"
      ],
      "metadata": {
        "id": "QOjvrxC4XNiH"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = get_propositions(test_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "id": "rHEKMQt1Wn84",
        "outputId": "8d016a36-47b0-452b-8779-608bb6d53845"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "error",
          "ename": "BadRequestError",
          "evalue": "Error code: 400 - {'error': {'message': \"Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.\", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': 'Here is the extracted information in JSON format:\\n\\n<tool-use>\\n{\\n  \"tool_calls\": [\\n    {\\n      \"id\": \"pending\",\\n      \"type\": \"function\",\\n      \"function\": {\\n        \"name\": \"information_extraction\",\\n        \"parameters\": {\\n          \"info\": {\\n            \"sentences\": [\\n              \"LARGE language models have achieved remarkable success, though they still face significant limitations.\",\\n              \"LARGE language models face significant limitations, especially in domain-specific or knowledge-intensive tasks.\",\\n              \"Domain-specific or knowledge-intensive tasks are challenging for LARGE language models.\",\\n              \"LARGE language models produce \\'hallucinations\\' when handling queries beyond their training data or requiring current information.\",\\n              \"Retrieval-Augmented Generation (RAG) enhances LARGE language models by retrieving relevant document chunks from external knowledge base through semantic similarity calculation.\",\\n              \"RAG effectively reduces the problem of generating factually incorrect content by referencing external knowledge.\",\\n              \"The integration of RAG into LARGE language models has resulted in widespread adoption.\",\\n              \"Index Terms —Large language model, retrieval-augmented generation, natural language processing, information retrieval\"\\n            ]\\n          }\\n        }\\n      }\\n    }\\n  ]\\n}\\n</tool-use>'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-72-6e31c726db91>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_propositions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-67-f9f276180240>\u001b[0m in \u001b[0;36mget_propositions\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      4\u001b[0m     }).content\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mpropositions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextraction_chain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrunnable_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpropositions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py\u001b[0m in \u001b[0;36mwarning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0mwarned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0memit_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mawarning_emitting_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    596\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"`run` supports only one positional argument.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m             return self(args[0], callbacks=callbacks, tags=tags, metadata=metadata)[\n\u001b[0m\u001b[1;32m    599\u001b[0m                 \u001b[0m_output_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m             ]\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py\u001b[0m in \u001b[0;36mwarning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0mwarned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0memit_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mawarning_emitting_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    379\u001b[0m         }\n\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m         return self.invoke(\n\u001b[0m\u001b[1;32m    382\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRunnableConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             outputs = (\n\u001b[0;32m--> 154\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallbackManagerForChainRun\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     ) -> Dict[str, str]:\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrun_manager\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseLanguageModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m             return self.llm.generate_prompt(\n\u001b[0m\u001b[1;32m    139\u001b[0m                 \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    774\u001b[0m     ) -> LLMResult:\n\u001b[1;32m    775\u001b[0m         \u001b[0mprompt_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 776\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    777\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m     async def agenerate_prompt(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mrun_managers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                     \u001b[0mrun_managers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_llm_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLLMResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m         flattened_outputs = [\n\u001b[1;32m    635\u001b[0m             \u001b[0mLLMResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mllm_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm_output\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[list-item]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    621\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m                 results.append(\n\u001b[0;32m--> 623\u001b[0;31m                     self._generate_with_cache(\n\u001b[0m\u001b[1;32m    624\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m                         \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    843\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 845\u001b[0;31m                 result = self._generate(\n\u001b[0m\u001b[1;32m    846\u001b[0m                     \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_groq/chat_models.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    470\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m         }\n\u001b[0;32m--> 472\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessage_dicts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    473\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_chat_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/groq/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    287\u001b[0m           \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOverride\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlevel\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseconds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \"\"\"\n\u001b[0;32m--> 289\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    290\u001b[0m             \u001b[0;34m\"/openai/v1/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1223\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m         )\n\u001b[0;32m-> 1225\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1227\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    918\u001b[0m         \u001b[0mstream_cls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_StreamT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m     ) -> ResponseT | _StreamT:\n\u001b[0;32m--> 920\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    921\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1016\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1018\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1019\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m         return self._process_response(\n",
            "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.\", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': 'Here is the extracted information in JSON format:\\n\\n<tool-use>\\n{\\n  \"tool_calls\": [\\n    {\\n      \"id\": \"pending\",\\n      \"type\": \"function\",\\n      \"function\": {\\n        \"name\": \"information_extraction\",\\n        \"parameters\": {\\n          \"info\": {\\n            \"sentences\": [\\n              \"LARGE language models have achieved remarkable success, though they still face significant limitations.\",\\n              \"LARGE language models face significant limitations, especially in domain-specific or knowledge-intensive tasks.\",\\n              \"Domain-specific or knowledge-intensive tasks are challenging for LARGE language models.\",\\n              \"LARGE language models produce \\'hallucinations\\' when handling queries beyond their training data or requiring current information.\",\\n              \"Retrieval-Augmented Generation (RAG) enhances LARGE language models by retrieving relevant document chunks from external knowledge base through semantic similarity calculation.\",\\n              \"RAG effectively reduces the problem of generating factually incorrect content by referencing external knowledge.\",\\n              \"The integration of RAG into LARGE language models has resulted in widespread adoption.\",\\n              \"Index Terms..."
          ]
        }
      ]
    }
  ]
}