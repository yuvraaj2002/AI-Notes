This page is dedicated towards understanding how we can actually evaluate our RAG pipelines and for that RAGAS is one of the framework. The link to official documentation is mentioned below but other than this all the things will be also be linked.

- [RAG evaluation framework](https://docs.ragas.io/en/stable/)
- [Evaluate your RAG playlist](https://youtube.com/playlist?list=PLrLEqwuz-mRI5ubqVJ7DpbHheCflJDDXk&si=tVDUknOpvjQfz4Z-)


- Quantitative performance metrics
- Qualitative performance metrics

How to build framework or system to evaluate the LLM and RAG

1. Model size and complexity
2. Quality of training data and diversity
3. Bias and fairness
4. Ethical considerations and responsible use
5. Fine tuning and transfer learning ? 
6. How much explainability is possible and traceability
7. Robustness and Adversarial attacks
8. Continuous monitoring and improvement

### Evaluations metrics

1. Embeddings based :Bert Score, Mover Score, WEAT (Intersection of language and embedding)
2. LLM assisted like RAGAS, Arize AI and GPT Score, G-Eval
3. Language based : NLI score, BLEU-RT and QA-QG Score
4. Word based and character based evaluation: ROGUE, BLEU, WER 
