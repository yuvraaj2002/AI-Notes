Tags : [[Generative AI]]

This page contains all my understanding about the History of the Large language models and the major resource used for these notes is the video of CampusX.

### Origin of LLM
Tags : [[Recurrent Neural Net]]

The Large language models which are also called as LLM have their foundations from the recurrent neural network, which are type of neural network that got specifically designed to work with the data in which sequence matters a lot, for example machine translation or time series data.

Now the thing is that RNN is having 4 major types which are 
1. One to Many
2. Many to One
3. Many to Many ( Synchronous )
4. Many to Many  ( Asynchronous )

Out of these 4 types of RNN, the Many to Many ( Asynchronous ) RNN interested the researchers a lot because of its usecases such as machine translation and text generation. So in order to solve these problems efficiently rather than using RNN or LSTM, a new architecture got introduced called Encoder-decoder architecture.

### Encoder Decoder architecture
Tags : [[Encoder Decoder]]

Encoder decoder architecture is basically a sequence to sequence model which takes the sequence of words as input and provide sequence of words as output. The most common usecase of such model is machine translation. Also the thing to mention is that in both encoder and decoder LSTM is being used rather than RNN.

Now even though this architecture seems promising but this model do not give us good results when we have a long text as input, in short it suffers from the long term dependencies and this occurs because in this architecture the context vector is the only vector which have to capture the context of the complete sentence.

### Attention Mechanism
Tags : [[Attention Model]]

To cover the drawback of Encoder decoder architecture suffering with the long term dependencies, attention mechanism got introduced where rather than putting the entire stress on single context vector to capture the context of complete text, the context vectors of every time step is provided to the decoder and because of that the decoder at every time step have the information of all the time steps and can utilize this information far better than only 1 context vector.

In between the encoder and decoder there is a layer called attention layer and the main role of this layer is to simply 
assign weights to each context vector and this allows the decoder to focus on most important piece of information required for the current time step. The overall process can be summarized as 

- **Context Vectors**: During the encoding phase, the encoder processes the input sequence and generates a sequence of context vectors (hidden states), one for each time step of the input.
    
- **Attention Mechanism**: At each time step of the decoding phase, the attention mechanism calculates attention weights for all the context vectors generated by the encoder. These weights represent the importance of each context vector with respect to the current decoding time step.
    
- **Weighted Sum**: The attention weights are used to compute a weighted sum of the context vectors. This weighted sum produces a new context vector that is specific to the current time step of the decoder.

Even though this approach seems most promising but this approach also have drawback and that is computational complexity. Since for everytime step the attention layer calculate the new context vector specific to that time step for the decoder, ( this overall process becomes very much computationally expensive in case we have long text ). Quadratic computation $O(N^2)$

Now to solve this researchers and a lot of research took place to figure out the way to make this thing less computationally expensive and finally the major point which got raised was that, the use of LSTM is the major roadblock in making the entire system less computationally expensive. The thing is that LSTM can process the data sequentially (one word at  a time), so if somehow this processing could be made parallel in that case the things will become very fast and then there comes Transformers.


### Transformers (Parallel processing)
Tags : [[Transformers]]

In transformers the core idea is that the LSTM cells are removed and it relies on the fundamental idea that attention is all you need and in both the encoder and decoder block instead of LSTM, only the attention is being used. Also since the transformers can do parallel data processing so they gave SOTA performance but the major issue with transformers was that it was not easy to train the transformers from scratch.

