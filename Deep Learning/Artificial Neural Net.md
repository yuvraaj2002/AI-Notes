This page is dedicated towards understanding the neural network. The resources that I have used for understanding this concept are the videos of CampusX majorly.


- What is neural network and from where did the neural network got inspired from ?
    
    Neural network is just a network of artificial neurons , where an artificial neuron is a combination of 2 mathematical functions ( Linear function ) and ( Activation function ) .
    
    ### From where did neural net got inspired from
    
    The neural network got inspired from the network of biological neurons present in our brain.

- What is an Artificial neuron?
    
    Artificial neuron is the smallest entity in the neural network , and is is just a mathematical function that perform certain operation 
    1. Calculating the weighted sum, then adding bias to it
    2. Passing it through activation function

- If we already had machine learning algorithms then why did we started using the neural nets ?
    
    There are many reasons behind why researchers started using neural nets instead of using machine learning algorithms ⬇️
    
    1. **Quantity of data**: Experimentally it is observed that the performance of machine learning algorithms somewhat becomes stable after certain amount of data whereas on the other hand the performance of neural nets improve as the amount of data increases.
    2. **Dimensionality of data**: In case there are a lot of features in the input data the performance of certain machine learning algorithms start decreasing due to the curse of dimensionality problem. On the other hand the neural nets are capable of dealing with data having high dimensionality.
    3. **Hierarchical feature extraction and transfer learning** : Neural nets because of the architecture are capable for extracting hierarchical features from the input data and due this the neural nets can be used for transfer learning we where we utilize the previous knowledge of the network for current use case.
- What are the major reasons which contributed in boom in deep learning or use of neural nets ?
    
    - Increase in data generation
    - Cheap and strong GPU availability
- What was the very first neural net architecture and explain it ?
    
    - Perceptron was the very first neural net architecture which got introduced. Basically this network had only 1 hidden layer with 1 artificial neuron.
    - In perceptron step function was used as activation function which gives only 2 values as n output that is 0 or 1 based on the threshold value.
    
    ![[Pasted image 20240701093018.png|500]]
    
    Even though the perceptron is a single-layer network with a single neuron, it laid the groundwork for more complex networks.

- What is the geometric intuition behind perceptron ?
    
    Perceptron is just a line that is used to divide the linear or sort of linear data into 2 classes or we can say ( 2 REGIONS ) because of which it is also called binary classifier.
    
    To better understand why perceptron is line , take a look at the basic structure of perceptron and you would observe that it is just an equation of line ( z = w1x1 + w2x2 + b ) can be represented as z = Ax + By + C
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/1bbc8410-e39e-4344-b69c-707be5169b6a/Untitled.png)
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/ee325cfb-dbe8-4ffd-a7b1-23aba19a6ca3/Untitled.png)
    
    <aside> ⬆️ In 2D perceptron act as line , In 3D perceptron act as plane and In 4D perceptron act as hyperplane
    
    </aside>
    
    But the drawback of perceptron is that incase we would be having a non linear data in that case the accuracy of the perceptron will be minimum in 2D, 3D and 4D as well
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/284951e2-e336-48a5-8b20-65d9fe67451f/Untitled.png)

- What was the drawback associated with single layer perceptron, what was done to deal with such drawback ?

	The major drawback of Perceptron was its inability to solve the complex problems in which data is not linearly separable and in order to deal with this issue a multi layer perceptron got introduced with new activation functions to learn complex pattern in the data.

- What do you mean by synapse in neural network ?
    
    The link between the neurons in the neural network is called synapse.

- What are learnable parameters , name them and how can we manipulate the output of the neural network ?
    
    _**Learnable parameters**_ are those parameters which are learned by the neural network during the training phase , and we can simply manipulate the output generated by the neural network by tweaking 2 learning parameters and which are weights and biases.
    
    In Neural network there are 2 learnable parameters :
    
    1. Weight : Weight is a learnable parameter that is used to define how much the neuron will activate.
        
        ![With the change in the weights angle of line is changing in case of perceptron](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/986b0ace-4977-4523-ad89-2a495fe1269c/Untitled.png)
        
        With the change in the weights angle of line is changing in case of perceptron
        
    2. Bias : Bias is a learnable parameter which is used in order add non linearity in neural network or we can say Bias is added to offset the result.
        
        ![Here as you can see with the change in bias the line is moving towards the origin in case of perceptron](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/0c1200f0-3283-4d88-ab18-9ff0023f369e/Untitled.png)
        
        Here as you can see with the change in bias the line is moving towards the origin in case of perceptron

- What does linear function and activation function do in neural network ?
    
    In a neural network, the linear function is responsible for both finding the weighted sum and adding the bias term. But if we will not be using non linear activation function then our neural net will simply behave like linear regression and will never be able to learn complex non linear relationship in the data.

- In how many directions does the data propagates or travel in the neural network architecture?
    
    2 directions and these 2 directions are forward and backward. Basically the forward propagation of data in the neural nets is for generating an output based on certain input, whereas the backward propagation of data is done to improve the model performance by finding those values of model parameters for which the value of loss function is minimum.
- How to forward propagation works in simple ANN ?
    
    Forward propagation is an algorithm which used to get predicted value as an output from the neural network based on certain input to the network.
    
    <aside> ➡️ In forward propagation the flow of data is from the input layer to output layer through hidden layer
    
    </aside>
    
    In order to better understand what happens during the forward propagation here are the few points to keep in mind ⬇️
    
    1. First of all the neurons present in the input layer gets input in form of vector and then those input layer without applying any mathematical function pass the data to the hidden layer neurons.
    2. When the input layer neurons pass the data to the hidden layer neurons , then the weights also get initialized for each neuron in the hidden layer.
    3. After that the linear function present in every neuron of hidden layer computes the sum of product of input values and weights associated with every neurons of the previous layer neurons.
    4. After the linear function computes the sum of product of input values and weights , bias is added to the result and then the result is passed to the activation function , which decides that whether that neuron will activate or not .

- How does backward propagation works in ANN and why we do it ?
    
    After the forward propagation finishes executing the backward propagation algorithm start executing for training our neural network and to readjust the weights so that loss function or cost function could be reduced.
    
    <aside> ➡️ During the backward propagation the weights are readjusted using the formula ⬇️
    
    </aside>
    
    ![Just like the weight Updation formula , we have bias updation formula](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/44cd3a05-196c-4060-a3ff-0bbf0893cd7e/IMG_20220917_115229.jpg)
    
    Just like the weight Updation formula , we have bias updation formula
    
    Based on the polarity of the slope there are 2 cases ⬇️
    
    1. If slope → Negative then ( Wnew > Wold )
    2. If slope → Positive then ( Wnew < Wold )
    
    ![IMG_20220917_120509.jpg](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/5ac8c1ac-0a3c-404f-8a81-1155ff7caafa/IMG_20220917_120509.jpg)
    
    ![During backward propagation there can also be the situation like this ⬆️](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/b5317382-f9d0-4396-a8bc-00a5636a92d0/SmartSelect_20220917_121353_Edge.jpg)
    
    During backward propagation there can also be the situation like this ⬆️

- What is vanishing gradient problem ad how to solve it ?
    
    The thing is that incase we have deep neural network and in that neural network if we are using certain activation functions such as sigmoid activation function the derivative of sigmoid activation function value will be very small specifically within range form ( 0 to 0.25 )
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/344700b0-34e8-49be-a8c4-adee100aa444/Untitled.png)
    
    Now the problem is that during the back propagation when we will try to find the new updated weights using the chain rule differentiation then the value of derivative of loss function with respect to weight will be very small and after multiplying it with learning rate which itself is small value , then there will not be any significant change in the new updated weight value , which will not help the neural network to train properly.
    
    ![SmartSelect_20220917_134830_Edge.jpg](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/187fac7f-6f13-45e0-a84e-8b1871ee41b7/SmartSelect_20220917_134830_Edge.jpg)
    
    ### Solution
    
    The solution of such problem is to use some other activation functions

- What is the difference between loss function and cost function ?
    
    - Loss function is simply a mathematical function which is used for finding the difference between actual and predicted value for a single data point, whereas 
    - Cost function is like a superset of loss function and it gives the average value of loss function value for all the data points.
