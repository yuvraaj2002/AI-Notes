This page is dedicated towards understanding the neural network. The resources that I have used for understanding this concept are the videos of CampusX majorly.

### What do you mean by neurons and neural network ? 

Neurons also known as Artificial neurons are simply the mathematical model which are responsible for performing 2 mathematical operations

1. Calculating the weighted sum
2. Passing the weighted sum through activation function 

The network of such small mathematical model is called neural network.

### If we already had machine learning algorithms then why did we started using the neural nets ?

- Quantity of data: Experimentally it has been observed that after certain amount of data the performance of machine learning algorithms becomes stable, whereas performance of neural network tends to increase with increase in data
- High dimensional complex data : Neural network due to their complex architecture are capable of dealing with high dimensional complex data better than machine learning algorithms
- Hierarchical learning: Neural networks have the concept of hierarchical learning where, the initial layers of the neural network focus on very simpler and general features of the data whereas as the deep hidden layer focus on very niche or specific features. Now because of this hierarchical learning we can use the concept of transfer learning.

### What are the major reasons which contributed in boom in deep learning or use of neural nets ?

- Increase in data generation
- Cheap and strong GPU availability


### What do you mean by perceptron, its drawback and how it got solved ? 

Perceptron is also known as the the building block of the neural network and it is a simple single layer neural network which use step function as an activation function and can be used to solve only the binary classification problems. There are 2 main drawbacks associated with perceptron

1. It can only be used to solve binary classification problems
2. In binary classification problems as well it cannot solve the XOR problem, means it can't work with the data which is not linearly separable.

![[Deep Learning/Images/Perceptron.png|500]]

To solve the problems faced by perceptron, Multiple layer perceptron got introduced, where rather than using only single neuron in single layer, instead multiple layers started getting used with each layer having multiple neurons. Also rather than using step function we started using other activation functions such as sigmoid, Tanh and Relu etc. Now because of this increase in complexity the we were able to solve both classification problem, regression problem (Linearly or Non linearly separable data).


### What are learnable parameters , name them and how can we manipulate the output of the neural network ?

_**Learnable parameters**_ are those parameters which are learned by the neural network during the training phase , and we can simply manipulate the output generated by the neural network by tweaking 2 learning parameters and which are weights and biases.


### In how many directions does the data propagates or travel in the neural network architecture?

There are 2 directions in which the data flows in neural network and these 2 directions are

1. From Input layer towards Output layer : Forward pass or Forward propagation
2. From Output layer towards Input layer : Backward pass or Backward propagation

Forward propagation is responsible for making a prediction with given input and model parameters whereas the backward propagation is responsible for updating the model parameter values such that the error could be minimized.




### Explain what is vanishing and exploding gradient problem and how can we solve them ? 



- What is vanishing gradient problem ad how to solve it ?
    
    The thing is that incase we have deep neural network and in that neural network if we are using certain activation functions such as sigmoid activation function the derivative of sigmoid activation function value will be very small specifically within range form ( 0 to 0.25 )
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/344700b0-34e8-49be-a8c4-adee100aa444/Untitled.png)
    
    Now the problem is that during the back propagation when we will try to find the new updated weights using the chain rule differentiation then the value of derivative of loss function with respect to weight will be very small and after multiplying it with learning rate which itself is small value , then there will not be any significant change in the new updated weight value , which will not help the neural network to train properly.
    
    ![SmartSelect_20220917_134830_Edge.jpg](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/187fac7f-6f13-45e0-a84e-8b1871ee41b7/SmartSelect_20220917_134830_Edge.jpg)
    
    ### Solution
    
    The solution of such problem is to use some other activation functions

### What is the difference between loss function and cost function ?

- Loss function is simply a mathematical function which is used for finding the difference between actual and predicted value for a single data point, whereas 
- Cost function is like a superset of loss function and it gives the average value of loss function value for all the data points.
