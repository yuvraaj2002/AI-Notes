This page is dedicated towards the understanding of various optimizers in Deep learning and the resources that I will be using will be mentioned below.

### [Why to even study optimizers](#)

If we want to get the true understanding about the concept of optimizers then we need to be aware about the fact that why we even need to focus on this topic. Basically the topic of optimizers comes into the picture when we actually want to improve the performance of the neural network in terms of convergence speed because the traditional gradient descent and its variants have some major drawbacks 

1. Gradient descent is sensitive to the learning rate value
2. Model gets stuck in the local minima
3. Model converge slowly when we use gradient descent because in this algorithm we do not have the provision to use different learning rates for different parameters

on slope only but now we use the momentum,./