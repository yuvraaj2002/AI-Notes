After cleaning the raw text the next most important step is to make sure that we are providing correct numerical representation to the words, so that machines could efficiently capture the meaning of each word and also of the overall sentence. In this page we will first talk about the frequency based embedding techniques, followed by their drawback and finally about the prediction or neural network based embedding techniques.


### What do you mean by word embedding and what are the various approaches used ? 

Word embedding can simply be defined as the numerical representation of the word in form of vector and in order to create such vectors there are 2 main approaches which are being used

1. Frequency based techniques
2. Prediction or Neural Network Based techniques

The importance of word embeddings is that any machine learning or deep learning algorithms only understands numbers and since words are not in the correct format which such models expect so we need to give a numerical representation to such words so that machines could also understand what information is captured in the words.

### What do you mean by frequency based word embedding techniques and name them

Frequency based word embedding techniques are those techniques which utilize the frequency count of the word in the given document or vocabulary to provide the numerical representation to the word. There are 2 frequency based word embedding technique ⬇️

1. Bag Of Words
2. TF-IDF

### Explain working of BOW and its drawback  

### Explain working of TF-IDF and its drawback ? 

### In TF-IDF while calculating the IDF why we take log?

### What do you mean by prediction based word embedding techniques and name them

