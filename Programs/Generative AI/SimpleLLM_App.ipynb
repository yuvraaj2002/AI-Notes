{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49537129-e798-4675-bda2-d50de5caf743",
   "metadata": {},
   "source": [
    "## Installing the Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e0a11eaf-7538-49ad-bf64-c21a9d8a44db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain\n",
    "# !python -m pip install -qU langchain-groq --user\n",
    "# !pip install \"langserve[all]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c201379-8c70-4866-b145-a552192a5a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b2a1d4-2189-45db-acc4-a6c210470ea5",
   "metadata": {},
   "source": [
    "As your application becomes more complex, it can be challenging to track and debug the flow of information and LLM invocations. LangSmith is a tool designed to help with this. It allows you to inspect, monitor, and troubleshoot what’s happening inside your LangChain processes, providing better insights into how the chain or agent operates.\n",
    "\n",
    "The function getpass.getpass() prompts the user to enter the API key while keeping it hidden as they type (it won’t be shown in the terminal or notebook). This ensures that the sensitive information remains secure and is not accidentally exposed in logs or screen captures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcbb8f5e-7b12-4762-b523-bc0b384fdfb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "afb50904-a9b3-46c8-b15c-2ee2dc1f94bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"GROQ_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "53d72ab2-7b88-4edc-94fe-07e9af99bfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the LLM model\n",
    "model = ChatGroq(model=\"llama3-8b-8192\")\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00e85ec3-7065-4f00-8c4e-16fd9535ab34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Ciao!', response_metadata={'token_usage': {'completion_tokens': 4, 'prompt_tokens': 24, 'total_tokens': 28, 'completion_time': 0.003333333, 'prompt_time': 0.00244165, 'queue_time': 0.01293123, 'total_time': 0.005774983}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_a97cfe35ae', 'finish_reason': 'stop', 'logprobs': None}, id='run-c4f00614-d948-4589-acf8-abfc394d5c84-0', usage_metadata={'input_tokens': 24, 'output_tokens': 4, 'total_tokens': 28})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "\n",
    "    # To set the context for the AI model\n",
    "    SystemMessage(content=\"Translate the following from English into Italian\"),\n",
    "\n",
    "    # To represent the user input\n",
    "    HumanMessage(content=\"hi!\"),\n",
    "]\n",
    "\n",
    "model.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9eba73-3c19-4621-8002-5e2e9d58d712",
   "metadata": {},
   "source": [
    "Notice that the response from the model is an AIMessage. This contains a string response along with other metadata about the response. Oftentimes we may just want to work with the string response. We can parse out just this response by using a simple output parser."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1332246b-0d35-4b61-a2d7-de618f64cef8",
   "metadata": {},
   "source": [
    "There is a very common saying that Garbage in Garbage out and is is true for the LLM as well so if we want to get good quality response from the LLM then we will have to provide a good input prompt to the LLM. For doing so we can use the prompt templates which takes the raw user input and build a context around that input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a34a6fbf-0c96-476b-aa01-f185290659ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template = \"Translate the following into {language}:\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_template), (\"user\", \"{text}\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e250b3f4-3967-4f91-921d-0ebc7de20b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use the LangChain Expression Language (LCEL) to chain together LangChain modules\n",
    "chain = prompt_template | model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d13b631d-ae50-40d9-aa87-8fbb74586074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ਮੈਂ ਸਭ ਲਈ ਈਸੁ ਵਿੱਚ ਵਿਸ਼ਵਾਸ ਰੱਖਦਾ ਹਾਂ\\n\\n(Maen sabh laee Ees vich visvaas rakhandaa haan)\\n\\nTranslation: I believe in God for everything.'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"language\": \"Punjabi\", \"text\": \"I belive in God for everything\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tutorial_venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
