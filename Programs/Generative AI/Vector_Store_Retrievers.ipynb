{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "234b0f87-abaa-4c9d-8227-7fb1a2ae6875",
   "metadata": {},
   "source": [
    "## Installing the depedencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b557933-e6af-451a-80ea-350fabc8be08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -qU langchain-ollama\n",
    "# %pip install -qU langchain-community faiss-cpu\n",
    "# !python -m pip install -qU langchain-groq --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3674eea7-bbbc-4607-afa8-c4fca682add4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from uuid import uuid4\n",
    "import getpass\n",
    "from langchain_core.documents import Document\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "import faiss\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.runnables import RunnableLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2096d22f-d668-4e7c-8250-329345bc529a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6f9f3c-7025-4aba-a206-2393ee5357ed",
   "metadata": {},
   "source": [
    "LangChain implements a Document abstraction, which is intended to represent a unit of text and associated metadata. It has two attributes:\n",
    "\n",
    "- page_content: a string representing the content;\n",
    "- metadata: a dict containing arbitrary metadata.\n",
    "\n",
    "Here is the link to index of various embedding stores : https://python.langchain.com/v0.2/docs/integrations/vectorstores/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ca76182-550f-4a00-9792-a27286092113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a list of Document objects\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=\"Dogs are great companions, known for their loyalty and friendliness.\",\n",
    "        metadata={\"source\": \"mammal-pets-doc\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Cats are independent pets that often enjoy their own space.\",\n",
    "        metadata={\"source\": \"mammal-pets-doc\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Goldfish are popular pets for beginners, requiring relatively simple care.\",\n",
    "        metadata={\"source\": \"fish-pets-doc\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Parrots are intelligent birds capable of mimicking human speech.\",\n",
    "        metadata={\"source\": \"bird-pets-doc\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Rabbits are social animals that need plenty of space to hop around.\",\n",
    "        metadata={\"source\": \"mammal-pets-doc\"},\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184d99a0-bf6f-4cb3-88f2-fcb60f395a0c",
   "metadata": {},
   "source": [
    "## Loading the Embedding Model\n",
    "When using Ollama embeddings with the langchain_ollama integration, you first need to download the model locally. Here's how it works:\n",
    "\n",
    "1. Download the Model: You must have the Ollama model downloaded locally. Ollama provides language models that are accessed locally, meaning the model files reside on your machine rather than being fetched from a remote server during runtime.\n",
    "\n",
    "2. Use langchain_ollama: Once the model is downloaded, you can then load it using the langchain_ollama integration to generate embeddings or perform other operations.\n",
    "\n",
    "This approach is common for ensuring that the model runs efficiently and offline, with all computations being performed locally. You need to manage the initial setup (like downloading the model) before integrating it with LangChain or other libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a12807c3-b766-4dd2-8948-6852345214b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = OllamaEmbeddings(\n",
    "    model=\"llama3\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c775f8-df25-44b9-bf13-17b64143d02a",
   "metadata": {},
   "source": [
    "FAISS is primarily in-memory by default. The index is stored in RAM, which allows for fast similarity search and retrieval operations. However, FAISS can also handle larger datasets that don’t fit entirely in memory by using techniques like on-disk indices.\n",
    "\n",
    "Key points:\n",
    "\n",
    "- In-memory: By default, FAISS stores indices in memory for high-speed retrieval.\n",
    "- On-disk storage: FAISS provides options to save indices to disk and reload them later, enabling the handling of larger datasets beyond memory constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "218344da-442d-41c4-9dce-693ab9386853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an index \n",
    "index = faiss.IndexFlatL2(len(embedding_model.embed_query(\"hello world\"))) # Length = 4096\n",
    "\n",
    "vector_store = FAISS(\n",
    "    embedding_function=embedding_model,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c2933958-a910-4212-a5fe-80c2aa234bdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a6b9696b-5ed4-4302-96ae-32b390cf7ba7',\n",
       " '35d2e53c-00cb-431b-a932-7efe11198cd6',\n",
       " '47a7be51-d0f6-4930-a31f-8a0c39667dc5',\n",
       " '09df7b0c-ff71-42e6-8d10-f6aabf8236eb',\n",
       " 'ea154ec7-adde-4fb9-8d86-af01f37eb89f']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generating unique ids for the documents\n",
    "uuids = [str(uuid4()) for _ in range(len(documents))]\n",
    "\n",
    "vector_store.add_documents(documents=documents, ids=uuids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f35399-ade4-4e07-b8fa-fc749581c775",
   "metadata": {},
   "source": [
    "VectorStore and Runnable: In LangChain, VectorStore objects (used to store and search for vectors, like embeddings) do not automatically work with the LangChain Expression Language (LCEL) because they don't have the necessary methods to \"run\" as part of a chain.\n",
    "\n",
    "Retrievers as Runnables: Retrievers are special components in LangChain that can be easily connected into a chain because they follow a standard set of methods (like calling them synchronously, asynchronously, or in batches). These methods make them compatible with LCEL chains.\n",
    "\n",
    "Creating a Runnable for VectorStore: Even though VectorStore isn’t directly runnable, you can create a simple wrapper (a runnable) around it by deciding how you want to retrieve documents (like using the similarity_search method). This allows you to use VectorStore in LCEL chains without needing to fully subclass or extend it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c1c486c3-c0c1-44ec-8863-54ed81ac159e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(metadata={'source': 'mammal-pets-doc'}, page_content='Dogs are great companions, known for their loyalty and friendliness.')],\n",
       " [Document(metadata={'source': 'fish-pets-doc'}, page_content='Goldfish are popular pets for beginners, requiring relatively simple care.')]]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 1},\n",
    ")\n",
    "\n",
    "retriever.batch([\"cat\", \"shark\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b4ce4b-1e7f-47e4-8df6-89dfdf0149f5",
   "metadata": {},
   "source": [
    "Further we can put things into a chain along with the Large Language Model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
